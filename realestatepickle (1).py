# -*- coding: utf-8 -*-
"""RealEstatePickle.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13UdcyM2u3Erm1gpFVtI9XsvnVD4wz7aj

<img src="https://upload.wikimedia.org/wikipedia/en/5/5e/Munster_Technological_University_Logo%2C_2021.jpg">

> Orlando Jr.



> Data Science and Analytics - MTU.



> Mr. Aengus Daly

##Part 3 - Hands on EDA and Statical Modeling. In the end we will create a PICKLE file with our model to Depoy and Run Online

##Libs Import
"""

#(EDA)
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import numpy as np
from scipy.stats import norm

#import streamlit as st
import joblib

#Models
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier

# Model Valuation
from sklearn.model_selection import train_test_split
#from sklearn.externals import joblib

import warnings
warnings.filterwarnings("ignore")

df = pd.read_csv("https://raw.githubusercontent.com/orlandojrps/stream/main/df_final.csv")

"""##Explanatory Data Analysis"""

#First Inspection
df.head()

#Rows and Columns
df.shape

rows, columns = df.shape

print(f"Number of Rows: {rows}")

print(f"Number of Columns: {columns}")

#Dtype Inspection
df.info()

df.city_area.unique()

#Summary 
df.describe()

"""##Initial Plots"""

df.hist(figsize=(12,12))
plt.show()

"""##As seen: in these initial plots, the majority of houses are combined up to 3 beds and 2 baths. In general, 100m2 and bellow are more common. """

#Missing Data?
df.isnull().sum()

"""##Correlation Matrix."""

#Correlation Matrix
corr = df.corr()
corr

#Correlation Plot
fig, ax = plt.subplots(figsize=(15,8))
sns.heatmap(corr, annot=True, ax=ax).set(title='Features Correlation');

"""##Not much to do here, Beds and Baths have small/medium correlation with price. Area seens more correlated"""

# Scatter Plots for Features
fig, ax = plt.subplots(figsize=(15,8))
sns.barplot(x=df['Beds'], y=df['Price'],ax=ax).set(title='Price x Beds');

"""###As the number of Beds increase in the locality, house prices goes up until 5 beds, then the effect reverses. This shows a weak positive correlation between these attributes."""

fig, ax = plt.subplots(figsize=(15,8))
sns.barplot(x=df['Baths'], y=df['Price'],ax=ax).set(title='Price X Baths');

"""###As the number of Baths increase in the locality, house prices goes up. This shows a better positive correlation between these attributes."""

fig, ax = plt.subplots(figsize=(15,8))
sns.scatterplot(x=df['Area'], y=df['Price'],hue=df["city_area"],ax=ax).set(title='Price x Area m2 and Cities');

"""## With this multivariate plot it's possible to see a trend, the blue dots(Cork South Central) presents moderate and high prices even for smaller areas"""

fig, ax = plt.subplots(figsize=(15,8))
sns.kdeplot(data=df, x="Price", hue="city_area").set(title="Distribution: Price x City Area");

fig, ax = plt.subplots(figsize=(15,8))
sns.kdeplot(data=df, x="Area", hue="city_area").set(title="Distribution: Area x City Area")

"""##Above 2 different Distribution Plots, they are particularly useful to compare diferente groups, analizing the distribution, skewness, median, std, etc. """

fig, ax = plt.subplots(figsize=(15,8))
sns.regplot(x=df['Area'], y=df['Price']).set(title="Linear Relationship: Area x Price");

"""##The trend line shows house price increasing with increase of house's areas.

"""

fig, ax = plt.subplots(figsize=(15,8))
sns.regplot(x=df['Beds'], y=df['Price']).set(title="Linear Relationship: Beds x Price");

"""##The trend line shows house price increasing with increase in Beds. Above 5 Beds the trend is missing. This finding may be very interesting, probably those observations corresponds to old houses or places far from Cork's Center.  """

fig, ax = plt.subplots(figsize=(15,8))
sns.regplot(x=df['Baths'], y=df['Price']).set(title="Linear Relationship: Baths x Price");

"""##Another positive correlation, more baths, higher prices. After for the trend weakens."""

# Create a 3D scatter plot of latitude, longitude, and price
fig = plt.figure(figsize=(10, 10))
ax = fig.add_subplot(111, projection='3d')
ax.scatter(df['Longitude'], df['Latitude'], df['Price'], s=50)

# Add labels and title
ax.set_xlabel('Longitude')
ax.set_ylabel('Latitude')
ax.set_zlabel('Price')
ax.set_title('3D Scatter Plot')

plt.show()

# Create a 3D scatter plot of latitude, longitude, and area
fig = plt.figure(figsize=(10, 10))
ax = fig.add_subplot(111, projection='3d')
ax.scatter(df['Beds'], df['Baths'], df['Price'], s=50)

# Add labels and title
ax.set_xlabel('Baths')
ax.set_ylabel('Beds')
ax.set_zlabel('Price')
ax.set_title('3D Scatter Plot')

plt.show()

"""### EDA FINDINGS 
1. Positive correlations beteween some Features, not much strong though.
2. Above 5 Beds prices drop abruptly, this may be due to houses placed far from city center of in bad condition. 
3. Houses in Cork - South Central have the most expensive m2. Higher prices in smaller areas

## Feature Engineering. To Improove the performance of the model we will create some agregations

## Average Price per City Area created
"""

# Calculate the average price by city area
mean_prices = df.groupby('city_area')['Price'].mean().sort_values()

# Create a bar plot of the mean prices by city area
fig, ax = plt.subplots(figsize=(15, 8))
ax.bar(mean_prices.index, mean_prices.values)
ax.set_xlabel('City Area')
ax.set_ylabel('Mean Price')
ax.set_title('Average Price by City Area')
plt.xticks(rotation=90)
plt.show()

"""##Average Price per m2"""

# Calculate the price per square meter
df['Price per m2'] = df['Price'] / df['Area']

# Calculate the average price per square meter by city area
mean_price_m2 = df.groupby('city_area')['Price per m2'].mean().sort_values()

# Print the result
print(mean_price_m2)

# Create a bar plot of the mean price per square meter by city area
fig, ax = plt.subplots(figsize=(10, 6))
ax.bar(mean_price_m2.index, mean_price_m2.values)
ax.set_xlabel('City Area')
ax.set_ylabel('Mean Price per m2')
ax.set_title('Average Price per m2 by City Area')
plt.xticks(rotation=90)
plt.show()

"""##Creating features: Price per m2, Mean Price per m2 by Area and Price per City Area"""

# Calculate the price per square meter
df['Price per m2'] = df['Price'] / df['Area']

# Calculate the average price per square meter by city area
mean_price_m2 = df.groupby('city_area')['Price per m2'].mean()

# Calculate the average price by city area
average_price = df.groupby('city_area')['Price'].mean()

# Add the mean price per square meter and average price by city area as new columns to the DataFrame
df['Mean Price per m2'] = df['city_area'].map(mean_price_m2)
df['Average Price by City Area'] = df['city_area'].map(average_price)

# Print the updated DataFrame
df.head()

"""##Outlier Detection"""

X = df[["Beds",	"Baths",	"Area","Price per m2", "Mean Price per m2", "Average Price by City Area"]]
y = df["Price"]

plt.figure(figsize=(13,5))

for feat, grd in zip(X, range(231,237)):
  plt.subplot(grd)
  sns.boxplot(y=df["Price"], color='grey')
  plt.ylabel('Value')
  plt.title('Boxplot\n%s'%feat)
plt.tight_layout()

# Calculate the IQR
Q1 = np.percentile(df["Price"], 25)
Q3 = np.percentile(df["Price"], 75)
IQR = Q3 - Q1

# Define the lower and upper bounds
lower_bound = Q1 - 1.5*IQR
upper_bound = Q3 + 1.5*IQR

# Filter out the outliers
df_clean = df[(df["Price"] > lower_bound) & (df["Price"] < upper_bound)]

plt.figure(figsize=(13,5))

for feat, grd in zip(df_clean, range(231,237)):
  plt.subplot(grd)
  sns.boxplot(y=df_clean["Price"], color='grey')
  plt.ylabel('Value')
  plt.title('Boxplot\n%s'%feat)
plt.tight_layout()

"""##Outliers Detected and Removed. Durig the modeling running multiple times the results without Outliers were always better, justifying the exclusion"""

df

X

y

"""##Train and Test Split"""

#Train-test split
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)
print(X_train.shape, X_test.shape)
print(y_train.shape, y_test.shape)

from statsmodels.graphics.gofplots import qqplot

qqplot(y,line='s')
plt.show()

"""##Although normality is not present in this case  we will proceed anyway,"""

from scipy.stats import skew

print(skew(y))

"""##With hight positive value, the feature is right skewed.

##Check Tranformations and its results in dataset
"""

# Scaling Data using Min-Max Scaler

from sklearn.preprocessing import MinMaxScaler
mms = MinMaxScaler()
X_train_norm = mms.fit_transform(X_train)
X_test_norm = mms.transform(X_test)

# Scaling Data using Standard Scaler

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X_train)
X_train_std = scaler.transform(X_train)
X_test_std = scaler.transform(X_test)

# Comparing Scaled Data

xx = np.arange(len(X_train_std))
yy1 = X_train_norm[:,0]
yy2 = X_train_std[:,0]
plt.scatter(xx,yy1,color='b')
plt.scatter(xx,yy2,color='r')

print(X_train_std.shape)
print(X_test_std.shape)

"""##Min-Max Scaler presents better results


"""

qqplot(X_test_norm,line='s')
plt.show()

"""## Model - OLS regression analysis"""

import statsmodels.api as sm
model_ols = sm.OLS(y_train, X_train_norm)
fitted = model_ols.fit()
print(fitted.summary())

# Make predictions on the test set
y_pred = fitted.predict(X_test_norm)

# Calculate evaluation metrics
from sklearn.metrics import mean_absolute_error, mean_squared_error
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)

# Print metrics
print(f'Mean Absolute Error: {mae:.2f}')
print(f'Mean Squared Error: {mse:.2f}')
print(f'Root Mean Squared Error: {rmse:.2f}')

"""##Implementing another regression"""

from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(X_train_norm, y_train)

# Print the intercept and coefficients
print('Intercept:', model.intercept_)
print('Coefficients:', model.coef_)

from sklearn.metrics import mean_absolute_error, mean_squared_error

# Predict on the test set
y_pred = model.predict(X_test_norm)

# Calculate metrics
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)

# Print metrics summary
print(f'Mean Absolute Error: {mae:.2f}')
print(f'Mean Squared Error: {mse:.2f}')
print(f'Root Mean Squared Error: {rmse:.2f}')

"""##Residual Tests"""

from scipy.stats import shapiro
fig, ax = plt.subplots(figsize=(16,4), ncols=2)
ax[0] = sns.scatterplot(x=y_train, y=fitted.resid, ax=ax[0])
ax[1] = sns.histplot(fitted.resid, ax=ax[1])

statistic, p_value = shapiro(fitted.resid)
if p_value>0.05:
    print("Distribution is normal. Statistic: {0:.3}, p-value: {1:.4}".format(statistic, p_value))
else:
    print("Distribution is not normal. Statistic: {0:.3}, p-value: {1:.4}".format(statistic, p_value))

"""## Testing a different Model to compare to Baseline"""

from sklearn.neighbors import KNeighborsRegressor as knn
model4 = knn(n_neighbors=3,p=1,algorithm='brute')
model4.fit(X_train_norm,y_train)

ypred3 = model4.predict(X_test_norm)
ypred3

model4.score(X_test_norm,y_test)

k_values = np.arange(1,100,2)

train_score_arr = []
val_score_arr = []

for k in k_values:
    
    model2 = knn(n_neighbors=k,p=1)
    model2.fit(X_train_norm,y_train)
    
    train_score = model2.score(X_train_norm, y_train) 
    train_score_arr.append(train_score*100)
    
    val_score = model2.score(X_test_norm, y_test)
    val_score_arr.append(val_score*100)
    
    print("k=%d, train_accuracy=%.2f%%, test_accuracy=%.2f%%" % (k, train_score * 100, val_score*100))

plt.plot(k_values,train_score_arr,'g')
plt.plot(k_values,val_score_arr,'r')

"""##No Clear Advantages in KNN

##Cross Fold Validation
"""

from sklearn.model_selection import cross_val_score 
cross_val_score_test = cross_val_score(model, X_test_std, y_test, cv=10, scoring="r2")
print(cross_val_score_test)

cross_val_score_test.mean()

from sklearn.metrics import r2_score

print(r2_score(y_test, ypred3))

"""##Not a fabulous results but enough for this stage"""

c = pd.DataFrame(ypred3, columns=['Estimated Price'])
c.head()

d = pd.DataFrame(y_test)
d = y_test.reset_index(drop=True)
d.head()

"""##Table Predict vs Observed"""

ynew = pd.concat([c,d], axis=1)
ynew

model.intercept_

predict = model.predict([[21662.46970267,  32388.10744395, 862416.06391603, 671735.05060649,
        -6279.20675687,  11941.06786216]])

predict

X

"""##Save the Model with Pickle to Deploy. 
Ref:https://betterprogramming.pub/pickling-machine-learning-models-aeb474bc2d78
"""

joblib.dump(model, 'model_lr_real_estate.pkl')

from io import BytesIO
import requests
mLink = 'https://github.com/orlandojrps/stream/blob/main/model_lr_real_estate.pkl?raw=true'
mfile = BytesIO(requests.get(mLink).content)

model = joblib.load(mfile)

print(model)

# Load the pre-trained linear regression model
lr_model = joblib.load(mfile)

"""##Some values inputed manually for testing the model. It works! Our model is working and ready to move to a Streamlib Web server."""

# Define the function to make a prediction
def predict(features):
    prediction = lr_model.predict(features)
    return prediction[0]
    
arr = np.array(	[1,4,	600,	2548.076923,	580555.569985,	362643.442623]).reshape(1, -1)

prediction = predict(	arr)

#c2.write('Your Suggested Price is:', 12)  
print(prediction)

"""# FROM THIS POINT JUST SOME EXPERIMENTS. """



!pip install numpy pandas geopandas matplotlib tilemapbase seaborn shapely requests rtree

!apt-get install -qq curl g++ make

!curl -L http://download.osgeo.org/libspatialindex/spatialindex-src-1.8.5.tar.gz | tar xz

# Commented out IPython magic to ensure Python compatibility.
# %cd spatialindex-src-1.8.5

!./configure; make; make install

!ldconfig

!pip install numpy pandas geopandas matplotlib tilemapbase seaborn shapely requests rtree

import numpy as np 
import pandas as pd
import geopandas as gpd 
from scipy.stats import gaussian_kde
import matplotlib.pyplot as plt
from matplotlib.font_manager import FontProperties
from matplotlib.path import Path
from matplotlib.textpath import TextToPath
import tilemapbase
import warnings
import matplotlib.cbook
import requests

import seaborn as sns
import shapely.speedups
shapely.speedups.enable()

import warnings
warnings.filterwarnings("ignore")
warnings.simplefilter(action='ignore', category=FutureWarning)

"""## Loading and preprocessing the data from Ireland
Create a new folder, change into the folder and download the data with wget:
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content
!mkdir amsterdam_data
# %cd amsterdam_data
!wget http://download.geofabrik.de/europe/ireland-and-northern-ireland-latest-free.shp.zip

"""###Unzip the files and load the points as a GeoDataFrame:

"""

!unzip "/content/amsterdam_data/ireland-and-northern-ireland-latest-free.shp.zip"

"""##Load Geodata from Cork

###then folder is Amsterdan but files are from Cork
"""

points = gpd.read_file("/content/amsterdam_data/gis_osm_pois_free_1.shp")
p2=gpd.read_file("/content/amsterdam_data/gis_osm_pois_free_1.shp")

"""##Load the shapefile for the city 

"""

city = gpd.read_file("https://raw.githubusercontent.com/orlandojrps/stream/main/cork-city-boundary.geojson")

city

points = points.to_crs({"init": "EPSG:4326"})

p2

p2.fclass.unique()

points.fclass.unique()

"""##Filter the data for restaurants (or any other POI category):



"""

pointsbkp=points.copy()

p3 = points.copy()
p3 = p3.to_crs("EPSG:3857")
p3[p3['fclass']=="police"]

p3['tmp'], city['tmp'] = 1, 1

df = pd.merge(p3, city, on='tmp', how='inner').drop(columns='tmp')

df[df.fclass=="police"]

# edit the figure size however you need to
plt.figure(num=None, figsize=(10,10), dpi=80, facecolor='w', edgecolor='k')
# create plot and axes
fig = plt.plot()
ax1 = plt.axes()
# these values can be changed as needed, the markers are LaTeX symbols
city.plot(ax=ax1, alpha=0.1, edgecolor="black", facecolor="white")
points.plot(ax=ax1, alpha = 0.1, color="red", marker='$\\bigtriangledown$',)
ax1.figure.savefig('/content/amsterdam_data/plot1.png', bbox_inches='tight')

"""## Add a background map to the plot

Load the background map using tilemapbase:
"""

from geopandas import GeoDataFrame
gdf = GeoDataFrame(points, geometry=points.geometry)
gdf.set_crs(epsg=4326, inplace=True,allow_override=True)

p3 = p3.to_crs("EPSG:3857")#importante
city = city.to_crs("EPSG:3857")#importante

tilemapbase.start_logging()
tilemapbase.init(create=True)
extent = tilemapbase.extent_from_frame(city, buffer = 25)

extent

fig, ax = plt.subplots(figsize=(10,10))

plotter = tilemapbase.Plotter(extent, tilemapbase.tiles.build_OSM(), width=400)
plotter.plot(ax)
city.plot(ax=ax, column="COUNTY")
#ax.legend(frame.name)
None

points

df[df['fclass']=="police"]

points

df.rename(columns={"geometry_x": "geometry"},inplace=True)

df[df['fclass']=="police"]

df.fclass.unique()

df2 = df.copy()
df2["fclass"].replace("police", "pol", inplace=True)
df2[df2["fclass"]=="pol"]

fig, ax = plt.subplots(figsize=(20,20))
plotter = tilemapbase.Plotter(extent, tilemapbase.tiles.build_OSM(), width=700)
plotter.plot(ax)
ax.set_xlim(-969429.7340171158,-922220.5893273839)
ax.set_ylim(6756239.91206337,6790000.430036527)
df2[df2["fclass"]=="doctors"].plot(ax=ax, alpha = 0.4, color="red", marker='$\\bigtriangledown$',)
ax.figure.savefig('/content/amsterdam_data/plot.png' )

xy = np.vstack([df2["geometry"].x,df2["geometry"].y])
z = gaussian_kde(xy)(xy)

fig, ax = plt.subplots(figsize=(10,10))
plotter = tilemapbase.Plotter(extent, tilemapbase.tiles.build_OSM(), width=1000)
plotter.plot(ax)
#ax.set_xlim(bounding_box[0]+2000, bounding_box[1])
#ax.set_ylim(bounding_box[2]+2000, bounding_box[3])
df2[df2["fclass"]=="doctors"].plot(ax=ax, alpha=0.3, edgecolor="black", facecolor="white")
ax.scatter(df2["geometry"].x, df2["geometry"].y, c=z, s=20, zorder=2, edgecolor='',  alpha=0.7)
ax.figure.savefig('/content/amsterdam_data/plot4.png', bbox_inches='tight')

fig, ax = plt.subplots(figsize=(10,10))
plotter = tilemapbase.Plotter(extent, tilemapbase.tiles.build_OSM(), width=1000)
plotter.plot(ax)
 
df2[df2["fclass"]=="doctors"].plot(ax=ax, alpha=0.3, edgecolor="black", facecolor="white")
sns.kdeplot(df2["geometry"].x, df2["geometry"].y, shade=True, alpha=0.5, cmap="viridis", shade_lowest=False)
ax.figure.savefig('/content/amsterdam_data/plot3.png', bbox_inches='tight')

xy = np.vstack([points["geometry"].x,points["geometry"].y])
z = gaussian_kde(xy)(xy)

"""##To be continued..."""